{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project: _Comment Volume Prediction using Neural Networks and Decision Trees_\n",
    "\n",
    "\n",
    "### Created by: _Nafiseh Asghari_\n",
    "\n",
    "\n",
    "## PART 2 : Train Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colname = [\"Page_Popularity\", \"Page_Checkins\", \"Page_talking_about\", \"Page_Category\",\n",
    "              \"CC1_min\", \"CC1_max\",\"CC1_avg\",\"CC1_median\",\"CC1_stdv\",\n",
    "               \"CC2_min\", \"CC2_max\",\"CC2_avg\",\"CC2_median\",\"CC2_stdv\",\n",
    "               \"CC3_min\", \"CC3_max\",\"CC3_avg\",\"CC3_median\",\"CC3_stdv\",\n",
    "               \"CC4_min\", \"CC4_max\",\"CC4_avg\",\"CC4_median\",\"CC4_stdv\",\n",
    "               \"CC5_min\", \"CC5_max\",\"CC5_avg\",\"CC5_median\",\"CC5_stdv\",\n",
    "               \"CC1\", \"CC2\",\"CC3\",\"CC4\",\"CC5\", \"Base_Time\", \"Post_Lenght\",\n",
    "               \"Post_Share\", \"Post_promo_Status\", \"H\", \"Post_published_sun\",\n",
    "               \"Post_published_mon\",\"Post_published_tue\",\"Post_published_wed\",\n",
    "               \"Post_published_thu\",\"Post_published_fri\",\"Post_published_sat\",\n",
    "               \"Base_DateTime_sun\",\"Base_DateTime_mon\",\"Base_DateTime_tue\",\n",
    "               \"Base_DateTime_wed\",\"Base_DateTime_thu\",\"Base_DateTime_fri\",\n",
    "               \"Base_DateTime_sat\" , \"Target\"]\n",
    "\n",
    "def read_file(variant_number): \n",
    "    \n",
    "    dataset = pd.read_csv(\"C:/Users/Nafiseh/my ipynb/Features_Variant_{}.csv\".format(variant_number), \n",
    "                          header = None, names = colname)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40949, 54)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import variant 1:\n",
    "dataset_1= read_file(1)\n",
    "dataset_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import test set:\n",
    "test = pd.read_csv(\"C:/Users/Nafiseh/Documents/UofT- ML/finalproject/Dataset/Testing/Features_TestSet.csv\",\n",
    "                   names = colname, header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data for Machine Learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import external module\n",
    "import ProjectModules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40949, 53), (40949,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train , y_train = ProjectModules.split_X_y(dataset_1)\n",
    "X_train.shape , y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10044, 53), (10044,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, y_test = ProjectModules.split_X_y(test)\n",
    "X_test.shape , y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 53), (100,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a test case with random 100 instances\n",
    "case1_test = ProjectModules.test_case(test, num_instances=100)\n",
    "X_case1, y_case1 = ProjectModules.split_X_y(case1_test)\n",
    "X_case1.shape , y_case1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hit@10\n",
    "\n",
    "def hit10 (y_test, y_pred):\n",
    "    # top 10 posts that had received largest number of comments in actual\n",
    "    top10_real= y_test.argsort()[::-1][:10]\n",
    "    # top 10 posts that were predicted to have largest number of comments\n",
    "    top10_pred= y_pred.argsort()[::-1][:10]\n",
    "    # count how many of these 2 sets have matched\n",
    "    common_top10= np.array(list(set(top10_pred).intersection(top10_real)))\n",
    "    return len(common_top10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#AUC@10\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def auc10 (y_test, y_pred):\n",
    "    # top 10 posts that had received largest number of comments in actual\n",
    "    top10_real= y_test.argsort()[::-1][:10]\n",
    "    # define a new binary set which is 1 for top 10 and zero for the rest\n",
    "    y_binary = [0]*len(y_test)\n",
    "    for i, j in enumerate (y_test):\n",
    "        if i in np.array(top10_real):\n",
    "            y_binary[i] = 1\n",
    "    # top 10 posts that were predicted to have largest number of comments\n",
    "    top10_pred= y_pred.argsort()[::-1][:10]\n",
    "    # define a new binary set which is 1 for top 10 and zero for the rest\n",
    "    y_binary_p = [0]*len(y_pred)\n",
    "    for i, j in enumerate (y_pred):\n",
    "        if i in np.array(top10_pred):\n",
    "            y_binary_p[i] = 1\n",
    "    return roc_auc_score(y_binary,y_binary_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## _1. Random Forest_\n",
    "\n",
    "### 1.1. Default Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    RandomForestRegressor: 9.6 seconds\n"
     ]
    }
   ],
   "source": [
    "# train RF with default hyper-parameters:\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "rf_reg= RandomForestRegressor(random_state = 42)\n",
    "rf_reg.fit(X_train,y_train)\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"    {}: {:.1f} seconds\".format(rf_reg.__class__.__name__, t2 - t1))\n",
    "time_taken_rf = t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(365)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def evaluation_metric (model):\n",
    "    MAE= []\n",
    "    HIT10 = []\n",
    "    AUC10= []\n",
    "    for case in range(10):\n",
    "\n",
    "        case = ProjectModules.test_case(test, num_instances=100)\n",
    "        X_case, y_case = ProjectModules.split_X_y(case)\n",
    "        y_pred= model.predict(X_case)\n",
    "        mae = mean_absolute_error(y_case, y_pred)\n",
    "        MAE.append(mae)\n",
    "        hit_10 = hit10(y_case ,y_pred)\n",
    "        HIT10.append(hit_10)\n",
    "        auc_10 = auc10(y_case ,y_pred)\n",
    "        AUC10.append(auc_10)\n",
    "    mae_test= round(mean_absolute_error(y_test, model.predict(X_test)),3)\n",
    "    print (\"MAE for the test set:\", mae_test)\n",
    "    print (\"AVG MAE for 10 random test cases: \" ,round(np.mean(MAE),3))\n",
    "    print (\"AVG HIT@10 for 10 random test cases: \", np.mean(HIT10))\n",
    "    print (\"AUC@10 for 10 random test cases: \" , round(np.mean(AUC10),2))    \n",
    "\n",
    "    return mae_test, MAE , HIT10, AUC10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for the test set: 31.3\n",
      "AVG MAE for 10 random test cases:  33.622\n",
      "AVG HIT@10 for 10 random test cases:  6.3\n",
      "AUC@10 for 10 random test cases:  0.79\n"
     ]
    }
   ],
   "source": [
    "mae_test_RF, MAE_RF, HIT10_RF,AUC10_RF = evaluation_metric (model = rf_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Tuned Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231.9 seconds\n"
     ]
    }
   ],
   "source": [
    "# train RF with tuned hyper-parameters:\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "\n",
    "param_distribs = {\n",
    "        \"n_estimators\": randint(low= 5, high = 100),\n",
    "        \"max_depth\":randint(low=2, high=10),\n",
    "        \"min_samples_split\":randint(low=20, high=2000)\n",
    "    }\n",
    "\n",
    "rnd_search_forest = RandomizedSearchCV(RandomForestRegressor(random_state = 42), param_distributions=param_distribs,\n",
    "                                n_iter=20, cv=5, scoring=\"neg_median_absolute_error\",n_jobs=-1)\n",
    "rnd_search_forest.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"{:.1f} seconds\".format(t2 - t1))\n",
    "time_taken_TRF = t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for the test set: 27.397\n",
      "AVG MAE for 10 random test cases:  29.257\n",
      "AVG HIT@10 for 10 random test cases:  5.2\n",
      "AUC@10 for 10 random test cases:  0.73\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(365)\n",
    "\n",
    "mae_test_TRF, MAE_TRF, HIT10_TRF,AUC10_TRF = evaluation_metric (model = rnd_search_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _2.MLP_\n",
    "\n",
    "### 2.1.  ONE hidden Layer,  4 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_norm = [\"Page_Popularity\", \"Page_Checkins\", \"Page_talking_about\", \n",
    "              \"CC1_min\", \"CC1_max\",\"CC1_avg\",\"CC1_median\",\"CC1_stdv\",\n",
    "               \"CC2_min\", \"CC2_max\",\"CC2_avg\",\"CC2_median\",\"CC2_stdv\",\n",
    "               \"CC3_min\", \"CC3_max\",\"CC3_avg\",\"CC3_median\",\"CC3_stdv\",\n",
    "               \"CC4_min\", \"CC4_max\",\"CC4_avg\",\"CC4_median\",\"CC4_stdv\",\n",
    "               \"CC5_min\", \"CC5_max\",\"CC5_avg\",\"CC5_median\",\"CC5_stdv\",\n",
    "               \"CC1\", \"CC2\",\"CC3\",\"CC4\",\"CC5\", \"Base_Time\", \"Post_Lenght\",\n",
    "               \"Post_Share\", \"H\"]\n",
    "\n",
    "X_train[cols_to_norm] = X_train[cols_to_norm].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "X_test[cols_to_norm] = X_test[cols_to_norm].apply(lambda x: (x - x.min()) / (x.max() - x.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nafiseh\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Continuous Features:\n",
    "    \n",
    "num_features = [\"Page_Popularity\", \"Page_Checkins\", \"Page_talking_about\",\n",
    "              \"CC1_min\", \"CC1_max\",\"CC1_avg\",\"CC1_median\",\"CC1_stdv\",\n",
    "               \"CC2_min\", \"CC2_max\",\"CC2_avg\",\"CC2_median\",\"CC2_stdv\",\n",
    "               \"CC3_min\", \"CC3_max\",\"CC3_avg\",\"CC3_median\",\"CC3_stdv\",\n",
    "               \"CC4_min\", \"CC4_max\",\"CC4_avg\",\"CC4_median\",\"CC4_stdv\",\n",
    "               \"CC5_min\", \"CC5_max\",\"CC5_avg\",\"CC5_median\",\"CC5_stdv\",\n",
    "               \"CC1\", \"CC2\",\"CC3\",\"CC4\",\"CC5\", \"Base_Time\", \"Post_Lenght\",\n",
    "               \"Post_Share\", \"H\"]\n",
    "\n",
    "num_feature_cols = [tf.feature_column.numeric_column(k) for k in num_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Categorical Features:\n",
    "    # 1. binary Features:\n",
    "cat_features = [\n",
    "               \"Post_promo_Status\",  \"Post_published_sun\",\n",
    "               \"Post_published_mon\",\"Post_published_tue\",\"Post_published_wed\",\n",
    "               \"Post_published_thu\",\"Post_published_fri\",\"Post_published_sat\",\n",
    "               \"Base_DateTime_sun\",\"Base_DateTime_mon\",\"Base_DateTime_tue\",\n",
    "               \"Base_DateTime_wed\",\"Base_DateTime_thu\",\"Base_DateTime_fri\",\n",
    "               \"Base_DateTime_sat\" ]\n",
    "\n",
    "cat_feature_cols = [tf.feature_column.categorical_column_with_identity(k, num_buckets=2) for k in cat_features]\n",
    "                    \n",
    "\n",
    "    # 2. page category features:\n",
    "Page_Category = tf.feature_column.categorical_column_with_identity(\"Page_Category\", num_buckets=106, default_value=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indicator_cat_cols= [tf.feature_column.indicator_column(k) for k in cat_feature_cols]\n",
    "indicator_Page_Category = [tf.feature_column.indicator_column(Page_Category)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = num_feature_cols + indicator_cat_cols +indicator_Page_Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Tensorflow 1.6.0\n",
      "INFO:root:Saving to ./DNNRegressors_MLP1L/4_\n",
      "INFO:root:Train the DNN Regressor...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 138167.45312 loss\n",
      "Epoch 11: 133711.46875 loss\n",
      "Epoch 21: 131893.31250 loss\n",
      "Epoch 31: 132564.64062 loss\n",
      "Epoch 41: 131284.68750 loss\n",
      "Epoch 51: 130148.89844 loss\n",
      "Epoch 61: 129524.93750 loss\n",
      "Epoch 71: 129390.22656 loss\n",
      "Epoch 81: 128452.21094 loss\n",
      "Epoch 91: 127785.95312 loss\n",
      "Epoch 101: 127945.39062 loss\n",
      "Epoch 111: 127840.60938 loss\n",
      "Epoch 121: 127329.77344 loss\n",
      "Epoch 131: 127177.15625 loss\n",
      "Epoch 141: 127299.17969 loss\n",
      "Epoch 151: 126832.76562 loss\n",
      "Epoch 161: 126266.88281 loss\n",
      "Epoch 171: 126555.24219 loss\n",
      "Epoch 181: 125976.48438 loss\n",
      "Epoch 191: 125885.37500 loss\n",
      "Epoch 201: 125878.05469 loss\n",
      "892.4 seconds\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.info('Tensorflow %s' % tf.__version__) \n",
    "\n",
    "\n",
    "# Defining the Tensorflow input functions\n",
    "# for training\n",
    "def training_input_fn(batch_size=1):\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "                x=X_train,\n",
    "                y=y_train ,\n",
    "                batch_size=batch_size,\n",
    "                num_epochs=None,\n",
    "                shuffle=True)\n",
    "# for test\n",
    "def test_input_fn():\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "                x=X_test,\n",
    "                y = y_test,\n",
    "                batch_size=10,\n",
    "                num_epochs=1,\n",
    "                shuffle=False)\n",
    "\n",
    "\n",
    "# Network Design\n",
    "# --------------\n",
    "feature_columns = num_feature_cols + indicator_cat_cols + indicator_Page_Category\n",
    "\n",
    "STEPS_PER_EPOCH = 10\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "hidden_layers = [4]\n",
    "#dropout = 0.0\n",
    "\n",
    "\n",
    "MODEL_PATH='./DNNRegressors_MLP1L/'\n",
    "for hl in hidden_layers:\n",
    "    MODEL_PATH += '%s_' % hl\n",
    "#MODEL_PATH += 'D0%s' % (int(dropout*10))\n",
    "logging.info('Saving to %s' % MODEL_PATH)\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "# Building the Network -- high-level API TF.Learn\n",
    "MLP_1Layer = tf.estimator.DNNRegressor(\n",
    "                hidden_units=hidden_layers,\n",
    "                feature_columns=feature_columns,\n",
    "                model_dir=MODEL_PATH)\n",
    "\n",
    "# Train it\n",
    "\n",
    "logging.info('Train the DNN Regressor...\\n')\n",
    "\n",
    "for epoch in range(EPOCHS+1):\n",
    "\n",
    "    # Fit the DNNRegressor \n",
    "    MLP_1Layer.train(input_fn=training_input_fn(batch_size=BATCH_SIZE),steps=STEPS_PER_EPOCH)\n",
    "\n",
    "\n",
    "    # Evaluate the DNNRegressor every 10th epoch\n",
    "    if epoch%10==0:\n",
    "        eval_dict = MLP_1Layer.evaluate(input_fn=test_input_fn())\n",
    "\n",
    "        print('Epoch %i: %.5f loss' % (epoch+1, eval_dict['loss']))\n",
    "\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"{:.1f} seconds\".format(t2 - t1))\n",
    "time_taken_MLP1 = t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_input_func = tf.estimator.inputs.pandas_input_fn(\n",
    "      x=X_test,\n",
    "      batch_size=10,\n",
    "      num_epochs=1,\n",
    "      shuffle=False)\n",
    "\n",
    "pred_gen = MLP_1Layer.predict(predict_input_func)\n",
    "predictions_MLP1 = list(pred_gen)\n",
    "final_preds_MLP1 = []\n",
    "for pred in predictions_MLP1:\n",
    "    final_preds_MLP1.append(pred['predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for the test set: 29.433\n",
      "AVG MAE for 10 random test cases:  31.233\n",
      "AVG HIT@10 for 10 random test cases:  4.4\n",
      "AUC@10 for 10 random test cases:  0.69\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(365)\n",
    "MAE_MLP1 = []\n",
    "HIT10_MLP1=[]\n",
    "AUC10_MLP1 = []\n",
    "for c in range(10):\n",
    "    case_index= np.random.randint(0,len(test),100)\n",
    "    case =test.iloc[case_index]\n",
    "    y_pred= np.array(final_preds_MLP1)[case_index,0]\n",
    "    X_case, y_case = ProjectModules.split_X_y(case)\n",
    "    hit_10 = hit10(y_case ,y_pred)\n",
    "    HIT10_MLP1.append(hit_10)\n",
    "    auc_10 = auc10(y_case ,y_pred)\n",
    "    AUC10_MLP1.append(auc_10)\n",
    "    mae = mean_absolute_error(y_case, y_pred)\n",
    "    MAE_MLP1.append(mae)\n",
    "mae_test_MLP1=round(mean_absolute_error(y_test,final_preds_MLP1),3)\n",
    "print (\"MAE for the test set:\", mae_test_MLP1)\n",
    "print (\"AVG MAE for 10 random test cases: \" ,round(np.mean(MAE_MLP1),3))\n",
    "print (\"AVG HIT@10 for 10 random test cases: \", np.mean(HIT10_MLP1))\n",
    "print (\"AUC@10 for 10 random test cases: \" , round(np.mean(AUC10_MLP1),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. TWO hidden Layer, 20 & 4 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Tensorflow 1.6.0\n",
      "INFO:root:Saving to ./DNNRegressors_MLP2L/20_4_D00\n",
      "INFO:root:Train the DNN Regressor...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 140683.67188 loss\n",
      "Epoch 11: 135773.23438 loss\n",
      "Epoch 21: 132286.81250 loss\n",
      "Epoch 31: 128862.76562 loss\n",
      "Epoch 41: 126485.79688 loss\n",
      "Epoch 51: 126507.51562 loss\n",
      "Epoch 61: 125298.60156 loss\n",
      "Epoch 71: 123265.28906 loss\n",
      "Epoch 81: 124012.88281 loss\n",
      "Epoch 91: 122993.92969 loss\n",
      "Epoch 101: 122307.85156 loss\n",
      "Epoch 111: 121447.19531 loss\n",
      "Epoch 121: 121298.13281 loss\n",
      "Epoch 131: 120180.82812 loss\n",
      "Epoch 141: 120809.44531 loss\n",
      "Epoch 151: 119687.96875 loss\n",
      "Epoch 161: 119164.31250 loss\n",
      "Epoch 171: 117804.25781 loss\n",
      "Epoch 181: 118184.74219 loss\n",
      "Epoch 191: 117486.56250 loss\n",
      "Epoch 201: 117532.82031 loss\n",
      "973.3 seconds\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.info('Tensorflow %s' % tf.__version__) \n",
    "\n",
    "\n",
    "# Defining the Tensorflow input functions\n",
    "# for training\n",
    "def training_input_fn(batch_size=1):\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "                x=X_train,\n",
    "                y=y_train ,\n",
    "                batch_size=batch_size,\n",
    "                num_epochs=None,\n",
    "                shuffle=True)\n",
    "# for test\n",
    "def test_input_fn():\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "                x=X_test,\n",
    "                y = y_test,\n",
    "                batch_size=10,\n",
    "                num_epochs=1,\n",
    "                shuffle=False)\n",
    "\n",
    "\n",
    "# Network Design\n",
    "# --------------\n",
    "feature_columns = num_feature_cols + indicator_cat_cols + indicator_Page_Category\n",
    "\n",
    "STEPS_PER_EPOCH = 10\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "hidden_layers = [20,4]\n",
    "dropout = 0.0\n",
    "\n",
    "MODEL_PATH='./DNNRegressors_MLP2L/'\n",
    "for hl in hidden_layers:\n",
    "    MODEL_PATH += '%s_' % hl\n",
    "MODEL_PATH += 'D0%s' % (int(dropout*10))\n",
    "logging.info('Saving to %s' % MODEL_PATH)\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "# Building the Network -- high-level API TF.Learn\n",
    "MLP_2Layer = tf.estimator.DNNRegressor(\n",
    "                hidden_units=hidden_layers,\n",
    "                feature_columns=feature_columns,\n",
    "                model_dir=MODEL_PATH,\n",
    "                dropout=dropout)\n",
    "\n",
    "# Train it\n",
    "\n",
    "logging.info('Train the DNN Regressor...\\n')\n",
    "\n",
    "for epoch in range(EPOCHS+1):\n",
    "\n",
    "    # Fit the DNNRegressor \n",
    "    MLP_2Layer.train(input_fn=training_input_fn(batch_size=BATCH_SIZE),steps=STEPS_PER_EPOCH)\n",
    "\n",
    "\n",
    "    # Evaluate the DNNRegressor every 10th epoch\n",
    "    if epoch%10==0:\n",
    "        eval_dict = MLP_2Layer.evaluate(input_fn=test_input_fn())\n",
    "\n",
    "        print('Epoch %i: %.5f loss' % (epoch+1, eval_dict['loss']))\n",
    "\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"{:.1f} seconds\".format(t2 - t1))\n",
    "time_taken_MLP2 = t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_input_func = tf.estimator.inputs.pandas_input_fn(\n",
    "      x=X_test,\n",
    "      batch_size=10,\n",
    "      num_epochs=1,\n",
    "      shuffle=False)\n",
    "\n",
    "pred_gen = MLP_2Layer.predict(predict_input_func)\n",
    "predictions_MLP2 = list(pred_gen)\n",
    "final_preds_MLP2 = []\n",
    "for pred in predictions_MLP2:\n",
    "    final_preds_MLP2.append(pred['predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for the test set: 28.147\n",
      "AVG MAE for 10 random test cases:  29.324\n",
      "AVG HIT@10 for 10 random test cases:  5.1\n",
      "AUC@10 for 10 random test cases:  0.73\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(365)\n",
    "MAE_MLP2 = []\n",
    "HIT10_MLP2=[]\n",
    "AUC10_MLP2 = []\n",
    "for c in range(10):\n",
    "    case_index= np.random.randint(0,len(test),100)\n",
    "    case =test.iloc[case_index]\n",
    "    y_pred= np.array(final_preds_MLP2)[case_index,0]\n",
    "    X_case, y_case = ProjectModules.split_X_y(case)\n",
    "    hit_10 = hit10(y_case ,y_pred)\n",
    "    HIT10_MLP2.append(hit_10)\n",
    "    auc_10 = auc10(y_case ,y_pred)\n",
    "    AUC10_MLP2.append(auc_10)\n",
    "    mae = mean_absolute_error(y_case, y_pred)\n",
    "    MAE_MLP2.append(mae)\n",
    "    \n",
    "mae_test_MLP2=round(mean_absolute_error(y_test,final_preds_MLP2),3)\n",
    "MAE_MLP2_avg = round(np.mean(MAE_MLP2),3)\n",
    "HIT10_MLP2_avg = np.mean(HIT10_MLP2)\n",
    "AUC10_MLP2_avg = round(np.mean(AUC10_MLP2),2)\n",
    "print (\"MAE for the test set:\", mae_test_MLP2)\n",
    "print (\"AVG MAE for 10 random test cases: \" ,round(np.mean(MAE_MLP2),3))\n",
    "print (\"AVG HIT@10 for 10 random test cases: \", np.mean(HIT10_MLP2))\n",
    "print (\"AUC@10 for 10 random test cases: \" , round(np.mean(AUC10_MLP2),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variant 1:\n",
    "\n",
    "compare_model= pd.DataFrame({\"M.A.E\": [mae_test_RF, mae_test_TRF, mae_test_MLP1,mae_test_MLP2],\n",
    "                             \"Hits @10\": [np.mean(HIT10_RF), np.mean(HIT10_TRF), np.mean(HIT10_MLP1),HIT10_MLP2_avg], \n",
    "                             \"AUC @10\" :[round(np.mean(AUC10_RF),3), round(np.mean(AUC10_TRF),3), round(np.mean(AUC10_MLP1),3),AUC10_MLP2_avg], \n",
    "                             \"Time Taken\": [time_taken_rf,time_taken_TRF,time_taken_MLP1,time_taken_MLP2]},\n",
    "                            index= [\"RF\", \"Tuned RF\",\"MLP (4)\", \"MLP(20,4)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC @10</th>\n",
       "      <th>Hits @10</th>\n",
       "      <th>M.A.E</th>\n",
       "      <th>Time Taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.794</td>\n",
       "      <td>6.3</td>\n",
       "      <td>31.300</td>\n",
       "      <td>9.559909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tuned RF</th>\n",
       "      <td>0.733</td>\n",
       "      <td>5.2</td>\n",
       "      <td>27.397</td>\n",
       "      <td>231.895602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP (4)</th>\n",
       "      <td>0.689</td>\n",
       "      <td>4.4</td>\n",
       "      <td>29.433</td>\n",
       "      <td>892.422689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP(20,4)</th>\n",
       "      <td>0.730</td>\n",
       "      <td>5.1</td>\n",
       "      <td>28.147</td>\n",
       "      <td>973.310087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           AUC @10  Hits @10   M.A.E  Time Taken\n",
       "RF           0.794       6.3  31.300    9.559909\n",
       "Tuned RF     0.733       5.2  27.397  231.895602\n",
       "MLP (4)      0.689       4.4  29.433  892.422689\n",
       "MLP(20,4)    0.730       5.1  28.147  973.310087"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Random Forest outperforms Neural Networks in the proposed comment volume prediction model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
